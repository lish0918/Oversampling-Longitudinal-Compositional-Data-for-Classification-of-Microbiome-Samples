{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cf98dd",
   "metadata": {},
   "source": [
    "Based on the full data subset A with no missing data, the full samples for y=0 and y=1 are learned and new samples are generated using the data and labels from tp2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1ccfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from skbio.stats.composition import clr, alr, ilr\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, cut_tree\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import gaussian_kde, gamma, dirichlet, multivariate_normal\n",
    "from scipy.special import softmax\n",
    "import scipy.stats as stats\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "#         'Accuracy': accuracy,\n",
    "        'Balanced_accuracy': balanced_accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "#         'F1 Score': f1,\n",
    "        'AUC': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2202c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb3d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b804814",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b4ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold cross-validation\n",
    "# 1. There are positive samples in both the training and validation sets.\n",
    "# 2. the class distributions in the training and validation sets are similar to the original dataset.\n",
    "\n",
    "n_splits = 5\n",
    "num_seeds = 5\n",
    "skf = StratifiedKFold(n_splits= n_splits, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc040e25",
   "metadata": {},
   "source": [
    "### metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b094fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv(\"data/BASIC_metadata_full.csv\", sep=',', low_memory=False)\n",
    "Metadata = Metadata.drop(Metadata.columns[0], axis=1)\n",
    "\n",
    "# convert timepoits to 0,1,2\n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester2\",\"TimePoint\"] = 0 \n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester3\",\"TimePoint\"] = 1\n",
    "Metadata.loc[Metadata.TimePoint == \"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "\n",
    "# turn insufficient reads to NaN\n",
    "i = Metadata[Metadata.ReadsNumber < 500000].index\n",
    "Metadata.loc[i, 'ReadsNumber'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8dc56",
   "metadata": {},
   "source": [
    "### species data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile =pd.read_csv(\"data/Species_Profile_full.csv\",sep=',',low_memory=False)\n",
    "\n",
    "# extract all bacteria names\n",
    "full_list_bacteria = list(profile.columns)[1:]\n",
    "\n",
    "species = profile.to_numpy()[:,1:]\n",
    "\n",
    "species_num = np.shape(species)[1] # 713 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9729239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join profile and metadeata\n",
    "merged_data_base = pd.merge(profile, Metadata, left_on='Sample_id', right_on='Sample_ID')\n",
    "\n",
    "merged_data = merged_data_base.dropna(subset=['ReadsNumber'])[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + full_list_bacteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09b0a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sample individuals whose EPDS != NaN at tp2\n",
    "individuals_with_na_epds_at_tp2 = merged_data[\n",
    "    (merged_data['TimePoint'] == 2) & (merged_data['EPDS'].isna())\n",
    "]['Individual_ID'].unique()\n",
    "\n",
    "data = merged_data[~merged_data['Individual_ID'].isin(individuals_with_na_epds_at_tp2)]\n",
    "\n",
    "# 2. Sample individuals with data at tp0, tp1 and tp2\n",
    "individuals_with_all_timepoints = data.groupby('Individual_ID').filter(lambda x: set(x['TimePoint']) >= {0, 1, 2})['Individual_ID'].unique()\n",
    "data = data[data['Individual_ID'].isin(individuals_with_all_timepoints)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c09730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that have a value of 0 at all time points for all samples\n",
    "columns_to_drop = []\n",
    "for col in full_list_bacteria:\n",
    "    if (data[col] == 0).all():\n",
    "        columns_to_drop.append(col)\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Update full_list_bacteria\n",
    "full_list_bacteria = [col for col in full_list_bacteria if col not in columns_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0e84f",
   "metadata": {},
   "source": [
    "### （1）Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "374f4ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Replace the zero value by 1/2 of the non - zero minimum value\n",
    "data[full_list_bacteria] = data[full_list_bacteria].astype(float)\n",
    "\n",
    "# Replace with 1/2 of the non-zero minimum value of the row\n",
    "matrix = data[full_list_bacteria].values\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    non_zero_values = row[~np.isnan(row) & (row > 0)]\n",
    "    if len(non_zero_values) > 0:\n",
    "        min_non_zero = np.min(non_zero_values)\n",
    "        half_min = min_non_zero / 2\n",
    "        row[row == 0] = half_min\n",
    "    matrix[i, :] = row\n",
    "\n",
    "data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c52966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    \n",
    "    row_sum = np.sum(row)\n",
    "    row = row / row_sum\n",
    "    matrix[i, :] = row\n",
    "\n",
    "data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d65d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = data.groupby('Individual_ID')\n",
    "\n",
    "transformed_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(full_list_bacteria)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[full_list_bacteria].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    transformed_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "transformed_data = np.array(transformed_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "735a337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority class： 15\n",
      "majority class： 70\n"
     ]
    }
   ],
   "source": [
    "print(\"minority class：\", len(labels[labels == 1]))\n",
    "print(\"majority class：\", len(labels[labels == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02162531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.5000 ± 0.0000\n",
      "  Sensitivity: 0.0000 ± 0.0000\n",
      "  Specificity: 1.0000 ± 0.0000\n",
      "  AUC: 0.5871 ± 0.1759\n"
     ]
    }
   ],
   "source": [
    "# Build an RF model on the data to make predictions with only CLR.\n",
    "# There are a lot of NAs in features\n",
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32012d48",
   "metadata": {},
   "source": [
    "### （2）Feature Extraction + OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00d7594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(train_data, k):\n",
    "    selected_features_tp0 = []\n",
    "    selected_features_tp1 = []\n",
    "    \n",
    "    for time_point in [0, 1]:\n",
    "        time_point_data = train_data[train_data['TimePoint'] == time_point]\n",
    "        \n",
    "        X = time_point_data.drop(['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'], axis=1)\n",
    "        y = time_point_data['Dichotomous_EPDS']\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=k) \n",
    "        X_new = selector.fit_transform(X, y)\n",
    "\n",
    "        feature_indices = selector.get_support(indices=True)\n",
    "        feature_columns = X.columns[feature_indices]\n",
    "        \n",
    "        if time_point == 0:\n",
    "            selected_features_tp0 = feature_columns\n",
    "        elif time_point == 1:\n",
    "            selected_features_tp1 = feature_columns\n",
    "            \n",
    "    feature_columns = list(set(selected_features_tp0) | set(selected_features_tp1))\n",
    "    print(f\"Selected features: {feature_columns}\")\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c75bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_datasets(data, feature_columns):\n",
    "    microbiome_columns = data.columns[4:]\n",
    "    \n",
    "    unused_bacteria_columns = [col for col in microbiome_columns if col not in feature_columns]\n",
    "    \n",
    "    others_column = data[unused_bacteria_columns].sum(axis=1).to_frame(name='Others')\n",
    "    \n",
    "    extracted_data = data[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + feature_columns]\n",
    "    \n",
    "    extracted_data = pd.concat([extracted_data, others_column], axis=1)\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6b8332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clr_transform(data):\n",
    "    data_copy = data.copy()\n",
    "    id_columns = data_copy.columns[:4]\n",
    "    microbiome_columns = data_copy.columns[4:]\n",
    "\n",
    "    for _, group in data_copy.groupby('TimePoint'):\n",
    "        indices = group.index\n",
    "        group_data = group[microbiome_columns].values\n",
    "        clr_values = clr(group_data)\n",
    "        data_copy.loc[indices, microbiome_columns] = clr_values\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ebb401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_extract_labels(data):\n",
    "    feature_columns = data.columns[4:]\n",
    "    \n",
    "    grouped = data.groupby('Individual_ID')\n",
    "\n",
    "    extracted_data_final = []\n",
    "    labels = []\n",
    "\n",
    "    for individual_id, group in grouped:\n",
    "        time_point_matrix = np.full((3, len(feature_columns)), np.nan)\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            time_point = int(row['TimePoint'])\n",
    "            time_point_matrix[time_point] = row[feature_columns].values\n",
    "\n",
    "        tp2_row = group[group['TimePoint'] == 2]\n",
    "        if not tp2_row.empty:  \n",
    "            label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "            labels.append(label)\n",
    "            extracted_data_final.append(time_point_matrix)\n",
    "\n",
    "    extracted_data_final = np.array(extracted_data_final)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return extracted_data_final, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d78f0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the number of new samples generated\n",
    "num_new_samples = 55 # 70-15 = 55\n",
    "original_num_time_steps = 2\n",
    "minority_class = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0cc07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_ids = np.unique(data['Individual_ID'])\n",
    "y_for_fold = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bced50",
   "metadata": {},
   "source": [
    "### cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09a329f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_shape, cond_dim, tp2_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.cond_dim = cond_dim\n",
    "        self.tp2_dim = tp2_dim\n",
    "\n",
    "        # Calculate input feature dimensions\n",
    "        input_dim = latent_dim + cond_dim + tp2_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  # Input layer\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 128),  # Hidden layer\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, np.prod(output_shape)),  # Output layer\n",
    "            nn.Tanh()  # Output range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z, c, tp2):\n",
    "        zctp2 = torch.cat([z, c, tp2], dim=1)\n",
    "        output = self.model(zctp2)\n",
    "        output = output.view(output.size(0), *self.output_shape) \n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape, cond_dim, tp2_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.cond_dim = cond_dim\n",
    "        self.tp2_dim = tp2_dim\n",
    "\n",
    "        # Calculate input feature dimensions\n",
    "        input_dim = np.prod(input_shape) + cond_dim + tp2_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),  # Input layer\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),  # Hidden layer\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1),  # Output layer\n",
    "            nn.Sigmoid()  # Output range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, tp2):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        xctp2 = torch.cat([x, c, tp2], dim=1)\n",
    "        return self.model(xctp2)\n",
    "    \n",
    "class cGAN:\n",
    "    def __init__(self, latent_dim, output_shape, cond_dim, tp2_dim, lr=0.0002, b1=0.5, b2=0.999):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_shape = output_shape\n",
    "        self.cond_dim = cond_dim\n",
    "        self.tp2_dim = tp2_dim\n",
    "\n",
    "        # Initialize generator and discriminator\n",
    "        self.generator = Generator(latent_dim, output_shape, cond_dim, tp2_dim)\n",
    "        self.discriminator = Discriminator(output_shape, cond_dim, tp2_dim)\n",
    "\n",
    "        # Define optimizers\n",
    "        self.optimizer_G = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        self.optimizer_D = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "        # Define loss function\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "    def train(self, X_train, y_train, tp2_train, epochs, batch_size):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "        tp2_train = torch.tensor(tp2_train, dtype=torch.float32)\n",
    "\n",
    "        y_train_onehot = torch.nn.functional.one_hot(y_train.long(), num_classes=self.cond_dim).float()\n",
    "\n",
    "        dataset = TensorDataset(X_train, y_train_onehot, tp2_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for real_data, real_c, real_tp2 in dataloader:\n",
    "                # Labels for real samples\n",
    "                real_labels = torch.ones(real_data.size(0), 1)\n",
    "                # Generate fake samples\n",
    "                noise = torch.randn(real_data.size(0), self.latent_dim)\n",
    "                fake_data = self.generator(noise, real_c, real_tp2)\n",
    "                # Labels for fake samples\n",
    "                fake_labels = torch.zeros(real_data.size(0), 1)\n",
    "\n",
    "                # Train discriminator\n",
    "                self.optimizer_D.zero_grad()\n",
    "                real_loss = self.loss_fn(self.discriminator(real_data, real_c, real_tp2), real_labels)\n",
    "                fake_loss = self.loss_fn(self.discriminator(fake_data.detach(), real_c, real_tp2), fake_labels)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "\n",
    "                # Train generator\n",
    "                self.optimizer_G.zero_grad()\n",
    "                g_loss = self.loss_fn(self.discriminator(fake_data, real_c, real_tp2), real_labels)\n",
    "                g_loss.backward()\n",
    "                self.optimizer_G.step()\n",
    "\n",
    "    def generate_samples(self, num_samples, minority_class, tp2_samples):\n",
    "        self.generator.eval()\n",
    "        # Generate class labels\n",
    "        new_c = torch.ones(num_samples, dtype=torch.long) * minority_class\n",
    "        new_c = torch.nn.functional.one_hot(new_c, num_classes=self.cond_dim).float()\n",
    "        # Generate random noise\n",
    "        noise = torch.randn(num_samples, self.latent_dim)\n",
    "        # Convert tp2_samples to torch.Tensor\n",
    "        tp2_samples = torch.tensor(tp2_samples, dtype=torch.float32)\n",
    "        # Generate new samples\n",
    "        with torch.no_grad():\n",
    "            fake_data = self.generator(noise, new_c, tp2_samples)\n",
    "        return fake_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bf45d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Pseudoflavonifractor_sp_An184', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Streptococcus_vestibularis', 'Enterococcus_faecalis', 'Streptococcus_sp_F0442', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Sellimonas_intestinalis', 'Clostridium_perfringens', 'Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Denitrobacterium_detoxificans', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Enterococcus_faecium', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila', 'Enterococcus_faecalis', 'Blautia_producta']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5795 ± 0.1299\n",
      "  Sensitivity: 0.2533 ± 0.2713\n",
      "  Specificity: 0.9057 ± 0.0721\n",
      "  AUC: 0.6552 ± 0.1213\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    extracted_data = clr_transform(extracted_data)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    original_num_features = extracted_data_final.shape[2]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1) \n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    tp2_minority = tp2_train[minority_indices]\n",
    "    \n",
    "    repeat_times = num_new_samples // len(tp2_minority) + 1\n",
    "    tp2_minority = np.tile(tp2_minority, (repeat_times, 1))[:num_new_samples]\n",
    "\n",
    "    latent_dim = 100\n",
    "    output_shape = (original_num_time_steps, original_num_features)\n",
    "    cond_dim = 2  # two classes\n",
    "    cgan = cGAN(latent_dim, output_shape, cond_dim, tp2_train.shape[1])  \n",
    "\n",
    "    cgan.train(X_train, y_train, tp2_train, epochs=500, batch_size=16)\n",
    "\n",
    "    new_samples_np = cgan.generate_samples(num_new_samples, minority_class=1, tp2_samples=tp2_minority)\n",
    "    new_samples_np = new_samples_np.reshape(num_new_samples, -1)\n",
    "\n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(num_new_samples, minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c35ea",
   "metadata": {},
   "source": [
    "### cGMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dbf8973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGaussianMixtureModel:\n",
    "    \n",
    "    def __init__(self, n_components=2, max_iter=100, tol=1e-3, random_state=None): \n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def _initialize_parameters(self, X, C):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_conditions = C.shape[1]\n",
    "        \n",
    "        # Initialize using K-means\n",
    "        kmeans = KMeans(n_clusters=self.n_components, random_state=self.random_state)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Initialize weight parameters (related to conditional variables)\n",
    "        self.weight_coeffs = np.random.randn(self.n_components, n_conditions + 1)\n",
    "        \n",
    "        # Initialize mean parameters (related to conditional variables)\n",
    "        self.mean_coeffs = np.zeros((self.n_components, n_features, n_conditions + 1))\n",
    "        for k in range(self.n_components):\n",
    "            mask = (labels == k)\n",
    "            if np.sum(mask) > 0:\n",
    "                self.mean_coeffs[k, :, 0] = np.mean(X[mask], axis=0)\n",
    "        \n",
    "        # Initialize covariance parameters (now dependent on conditional variables)\n",
    "        # For each component, each unique element of covariance matrix has its own coefficients\n",
    "        self.n_cov_params = (n_features * (n_features + 1)) // 2  # Number of unique elements in symmetric matrix\n",
    "        self.cov_coeffs = np.zeros((self.n_components, self.n_cov_params, n_conditions + 1))\n",
    "        \n",
    "        # Initialize with empirical covariances\n",
    "        for k in range(self.n_components):\n",
    "            mask = (labels == k)\n",
    "            if np.sum(mask) > 0:\n",
    "                diff = X[mask] - self.mean_coeffs[k, :, 0]\n",
    "                emp_cov = np.dot(diff.T, diff) / np.sum(mask)\n",
    "            else:\n",
    "                emp_cov = np.eye(n_features)\n",
    "                \n",
    "            # Ensure positive definite\n",
    "            emp_cov += 1e-6 * np.eye(n_features)\n",
    "            \n",
    "            # Convert covariance matrix to vector form (Cholesky decomposition for stability)\n",
    "            try:\n",
    "                chol = np.linalg.cholesky(emp_cov)\n",
    "                cov_vec = self._chol_to_vec(chol)\n",
    "                self.cov_coeffs[k, :, 0] = cov_vec\n",
    "            except np.linalg.LinAlgError:\n",
    "                # If Cholesky fails, use identity\n",
    "                chol = np.eye(n_features)\n",
    "                cov_vec = self._chol_to_vec(chol)\n",
    "                self.cov_coeffs[k, :, 0] = cov_vec\n",
    "                \n",
    "    def _chol_to_vec(self, chol):\n",
    "    \"\"\"\n",
    "        Convert lower triangular Cholesky factor to vector\n",
    "    \"\"\"\n",
    "        n = chol.shape[0]\n",
    "        vec = []\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1):\n",
    "                vec.append(chol[i, j])\n",
    "        return np.array(vec)\n",
    "    \n",
    "    def _vec_to_chol(self, vec):\n",
    "    \"\"\"\n",
    "        Convert vector back to lower triangular Cholesky factor\n",
    "    \"\"\"\n",
    "        n = int((np.sqrt(1 + 8 * len(vec)) - 1) / 2)\n",
    "        chol = np.zeros((n, n))\n",
    "        idx = 0\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1):\n",
    "                chol[i, j] = vec[idx]\n",
    "                idx += 1\n",
    "        return chol\n",
    "    \n",
    "    def _compute_covariances(self, C_bias):\n",
    "    \"\"\"\n",
    "        Calculate covariance matrices for each sample under each component\n",
    "    \"\"\"\n",
    "        n_samples = C_bias.shape[0]\n",
    "        n_features = self.mean_coeffs.shape[1]\n",
    "        \n",
    "        covariances = np.zeros((n_samples, self.n_components, n_features, n_features))\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            # Compute covariance coefficients for each sample\n",
    "            cov_vecs = np.dot(C_bias, self.cov_coeffs[k].T)  # (n_samples, n_cov_params)\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                # Convert vector back to Cholesky factor\n",
    "                chol = self._vec_to_chol(cov_vecs[i])\n",
    "                \n",
    "                # Ensure diagonal elements are positive for numerical stability\n",
    "                for d in range(n_features):\n",
    "                    if chol[d, d] <= 0:\n",
    "                        chol[d, d] = 1e-6\n",
    "                \n",
    "                # Compute covariance matrix as Chol * Chol^T\n",
    "                covariances[i, k] = np.dot(chol, chol.T)\n",
    "                \n",
    "                # Add regularization for numerical stability\n",
    "                covariances[i, k] += 1e-8 * np.eye(n_features)\n",
    "                \n",
    "        return covariances\n",
    "    \n",
    "    def _e_step(self, X, C):\n",
    "    \"\"\"\n",
    "        E-step: Calculate posterior probability of each sample belonging to each component\n",
    "    \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        log_prob = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        # Calculate weight for each component (dependent on conditional variables)\n",
    "        C_bias = np.hstack([np.ones((n_samples, 1)), C])\n",
    "        weights = self._compute_weights(C_bias)\n",
    "        \n",
    "        # Calculate mean for each component (dependent on conditional variables)\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        # Calculate covariance for each component (dependent on conditional variables)\n",
    "        covariances = self._compute_covariances(C_bias)\n",
    "        \n",
    "        # Calculate log probability of each sample under each component\n",
    "        for k in range(self.n_components):\n",
    "            for i in range(n_samples):\n",
    "                try:\n",
    "                    log_prob[i, k] = np.log(weights[i, k] + 1e-10) + multivariate_normal.logpdf(\n",
    "                        X[i], mean=means[i, k], cov=covariances[i, k], allow_singular=True)\n",
    "                except:\n",
    "                    # If computation fails, use a default small probability\n",
    "                    log_prob[i, k] = np.log(weights[i, k] + 1e-10) - 1e6\n",
    "        \n",
    "        # Calculate posterior probabilities (responsibilities)\n",
    "        log_prob_max = np.max(log_prob, axis=1, keepdims=True)\n",
    "        log_prob -= log_prob_max\n",
    "        prob = np.exp(log_prob)\n",
    "        responsibilities = prob / (np.sum(prob, axis=1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, C, responsibilities):\n",
    "    \"\"\"\n",
    "        M-step: Update model parameters\n",
    "    \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_conditions = C.shape[1]\n",
    "        \n",
    "        # Add bias term for M-step\n",
    "        C_bias = np.hstack([np.ones((n_samples, 1)), C])\n",
    "        \n",
    "        # Update weight coefficients\n",
    "        for k in range(self.n_components):\n",
    "            # Set up weighted least squares problem\n",
    "            W = np.sqrt(responsibilities[:, k] + 1e-10)\n",
    "            y = W\n",
    "            X_weighted = W[:, np.newaxis] * C_bias\n",
    "            \n",
    "            # Solve weighted least squares problem\n",
    "            try:\n",
    "                self.weight_coeffs[k] = np.linalg.lstsq(X_weighted, y, rcond=None)[0]\n",
    "            except:\n",
    "                # If lstsq fails, keep previous values\n",
    "                pass\n",
    "        \n",
    "        # Update mean coefficients\n",
    "        for k in range(self.n_components):\n",
    "            for d in range(n_features):\n",
    "                # Set up weighted least squares problem\n",
    "                W = np.sqrt(responsibilities[:, k] + 1e-10)\n",
    "                y = W * X[:, d]\n",
    "                X_weighted = W[:, np.newaxis] * C_bias\n",
    "                \n",
    "                # Solve weighted least squares problem\n",
    "                try:\n",
    "                    self.mean_coeffs[k, d] = np.linalg.lstsq(X_weighted, y, rcond=None)[0]\n",
    "                except:\n",
    "                    # If lstsq fails, keep previous values\n",
    "                    pass\n",
    "        \n",
    "        # Update covariance coefficients (now dependent on conditional variables)\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            # For each covariance parameter, set up weighted least squares\n",
    "            for p in range(self.n_cov_params):\n",
    "                # First, we need to compute the target values for this parameter\n",
    "                target_values = np.zeros(n_samples)\n",
    "                \n",
    "                for i in range(n_samples):\n",
    "                    # Compute residual for this sample\n",
    "                    residual = X[i] - means[i, k]\n",
    "                    \n",
    "                    # Compute outer product\n",
    "                    outer_prod = np.outer(residual, residual)\n",
    "                    \n",
    "                    # Convert to Cholesky form and extract the p-th parameter\n",
    "                    try:\n",
    "                        chol_target = np.linalg.cholesky(outer_prod + 1e-6 * np.eye(n_features))\n",
    "                        target_vec = self._chol_to_vec(chol_target)\n",
    "                        target_values[i] = target_vec[p]\n",
    "                    except:\n",
    "                        # If Cholesky fails, use identity contribution\n",
    "                        chol_target = np.eye(n_features)\n",
    "                        target_vec = self._chol_to_vec(chol_target)\n",
    "                        target_values[i] = target_vec[p]\n",
    "                \n",
    "                # Set up weighted least squares problem\n",
    "                W = np.sqrt(responsibilities[:, k] + 1e-10)\n",
    "                y = W * target_values\n",
    "                X_weighted = W[:, np.newaxis] * C_bias\n",
    "                \n",
    "                # Solve weighted least squares problem\n",
    "                try:\n",
    "                    self.cov_coeffs[k, p] = np.linalg.lstsq(X_weighted, y, rcond=None)[0]\n",
    "                except:\n",
    "                    # If lstsq fails, keep previous values\n",
    "                    pass\n",
    "    \n",
    "    def _compute_weights(self, C_bias):\n",
    "    \"\"\"\n",
    "        Calculate weight for each sample under each component\n",
    "    \"\"\"\n",
    "        n_samples = C_bias.shape[0]\n",
    "        logits = np.dot(C_bias, self.weight_coeffs.T)\n",
    "        \n",
    "        # Use softmax to ensure weights are positive and sum to 1\n",
    "        logits_max = np.max(logits, axis=1, keepdims=True)\n",
    "        logits -= logits_max\n",
    "        exp_logits = np.exp(logits)\n",
    "        weights = exp_logits / (np.sum(exp_logits, axis=1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _compute_means(self, C_bias):\n",
    "    \"\"\"\n",
    "        Calculate mean for each sample under each component\n",
    "    \"\"\"\n",
    "        n_samples = C_bias.shape[0]\n",
    "        n_features = self.mean_coeffs.shape[1]\n",
    "        \n",
    "        means = np.zeros((n_samples, self.n_components, n_features))\n",
    "        for k in range(self.n_components):\n",
    "            for d in range(n_features):\n",
    "                means[:, k, d] = np.dot(C_bias, self.mean_coeffs[k, d])\n",
    "                \n",
    "        return means\n",
    "    \n",
    "    def fit(self, X, C):\n",
    "    \"\"\"\n",
    "        Fit conditional Gaussian mixture model using EM algorithm\n",
    "    \"\"\"\n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X, C)\n",
    "        \n",
    "        # EM algorithm iterations\n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step: Calculate posterior probabilities\n",
    "            responsibilities = self._e_step(X, C)\n",
    "            \n",
    "            # M-step: Update parameters\n",
    "            self._m_step(X, C, responsibilities)\n",
    "            \n",
    "            # Calculate log likelihood\n",
    "            C_bias = np.hstack([np.ones((X.shape[0], 1)), C])\n",
    "            weights = self._compute_weights(C_bias)\n",
    "            means = self._compute_means(C_bias)\n",
    "            covariances = self._compute_covariances(C_bias)\n",
    "            \n",
    "            log_likelihood = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                component_log_probs = []\n",
    "                for k in range(self.n_components):\n",
    "                    try:\n",
    "                        component_log_prob = np.log(weights[i, k] + 1e-10) + multivariate_normal.logpdf(\n",
    "                            X[i], mean=means[i, k], cov=covariances[i, k], allow_singular=True)\n",
    "                    except:\n",
    "                        component_log_prob = np.log(weights[i, k] + 1e-10) - 1e6\n",
    "                    component_log_probs.append(component_log_prob)\n",
    "                \n",
    "                log_likelihood += self._log_sum_exp(np.array(component_log_probs))\n",
    "            \n",
    "            # Check for convergence\n",
    "            if abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                break\n",
    "                \n",
    "            prev_log_likelihood = log_likelihood\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _log_sum_exp(self, log_probs):\n",
    "    \"\"\"\n",
    "        Calculate log-sum-exp to avoid numerical overflow\n",
    "    \"\"\"\n",
    "        max_log_prob = np.max(log_probs)\n",
    "        return max_log_prob + np.log(np.sum(np.exp(log_probs - max_log_prob)) + 1e-10)\n",
    "    \n",
    "    def predict(self, X, C):\n",
    "    \"\"\"\n",
    "        Predict cluster labels for samples\n",
    "    \"\"\"\n",
    "        responsibilities = self._e_step(X, C)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X, C):\n",
    "    \"\"\"\n",
    "        Predict probability of samples belonging to each component\n",
    "    \"\"\"\n",
    "        return self._e_step(X, C)\n",
    "    \n",
    "    def sample(self, C, n_samples=1):\n",
    "    \"\"\"\n",
    "        Generate samples based on conditional variables\n",
    "    \"\"\"\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        \n",
    "        C_bias = np.hstack([np.ones((C.shape[0], 1)), C])\n",
    "        \\n",
    "        # Calculate mixture weights\n",
    "        weights = self._compute_weights(C_bias)\n",
    "        \n",
    "        # Calculate mean for each component\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        # Calculate covariance for each component\n",
    "        covariances = self._compute_covariances(C_bias)\n",
    "        \n",
    "        # Generate samples\n",
    "        samples = []\n",
    "        component_labels = []\n",
    "        \n",
    "        samples_per_condition = n_samples // C.shape[0]\n",
    "        remainder = n_samples % C.shape[0]\n",
    "        \n",
    "        for i in range(C.shape[0]):\n",
    "            # Determine number of samples to generate for current condition\n",
    "            n_current = samples_per_condition + (1 if i < remainder else 0)\n",
    "            \n",
    "            # Randomly select components\n",
    "            k_samples = np.random.choice(\n",
    "                self.n_components, \n",
    "                size=n_current, \n",
    "                p=weights[i]\n",
    "            )\n",
    "            \n",
    "            # Generate samples from each selected component\n",
    "            for k in k_samples:\n",
    "                try:\n",
    "                    sample = np.random.multivariate_normal(\n",
    "                        means[i, k], \n",
    "                        covariances[i, k]\n",
    "                    )\n",
    "                except:\n",
    "                    # If sampling fails, use identity covariance\n",
    "                    sample = np.random.multivariate_normal(\n",
    "                        means[i, k], \n",
    "                        np.eye(means.shape[2])\n",
    "                    )\n",
    "                samples.append(sample)\n",
    "                component_labels.append(k)\n",
    "                \n",
    "        return np.array(samples), np.array(component_labels)\n",
    "\n",
    "def conditional_gmm(X_train, y_train, tp2_train, minority_class=1, num_new_samples=100, n_components=2, random_state=42):\n",
    "    # Get indices of minority class samples\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    \n",
    "    # Create conditional variables: tp2 data + labels\n",
    "    C = np.column_stack([tp2_train, y_train.reshape(-1, 1)])\n",
    "    \n",
    "    # Create and fit conditional Gaussian mixture model\n",
    "    cgmm = ConditionalGaussianMixtureModel(n_components=n_components, random_state=random_state)\n",
    "    cgmm.fit(X_train, C)\n",
    "    \n",
    "    # Prepare conditions for generating samples\n",
    "    # Method 1: Use conditions from all minority class samples\n",
    "    minority_conditions = C[minority_indices]\n",
    "    \n",
    "    # If number of minority samples is less than requested new samples, reuse conditions\n",
    "    if len(minority_indices) < num_new_samples:\n",
    "        # Calculate number of repeats needed\n",
    "        repeat_times = num_new_samples // len(minority_indices) + 1\n",
    "        minority_conditions = np.tile(minority_conditions, (repeat_times, 1))[:num_new_samples]\n",
    "    else:\n",
    "        # Randomly select conditions from minority class samples\n",
    "        random_indices = np.random.choice(len(minority_indices), num_new_samples, replace=False)\n",
    "        minority_conditions = minority_conditions[random_indices]\n",
    "    \n",
    "    # Generate new samples using conditions\n",
    "    new_samples, _ = cgmm.sample(minority_conditions, n_samples=num_new_samples)\n",
    "    \n",
    "    # Label all generated samples as minority class\n",
    "    new_labels = np.full(num_new_samples, minority_class)\n",
    "    \n",
    "    return new_samples, new_labels"
   ]
 },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "909db420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Pseudoflavonifractor_sp_An184', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Streptococcus_vestibularis', 'Enterococcus_faecalis', 'Streptococcus_sp_F0442', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Sellimonas_intestinalis', 'Clostridium_perfringens', 'Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Denitrobacterium_detoxificans', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Enterococcus_faecium', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila', 'Enterococcus_faecalis', 'Blautia_producta']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.6414 ± 0.0456\n",
      "  Sensitivity: 0.6000 ± 0.1333\n",
      "  Specificity: 0.6829 ± 0.1526\n",
      "  AUC: 0.6176 ± 0.1226\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    extracted_data = clr_transform(extracted_data)\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "    \n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        auc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "        metrics = {\n",
    "            'Balanced_accuracy': balanced_accuracy,\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Specificity': specificity,\n",
    "            'AUC': auc,\n",
    "        }\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d742aaa",
   "metadata": {},
   "source": [
    "### CVAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c550309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, cond_dim, tp2_dim):\n",
    "        super(cVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.tp2_dim = tp2_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim + tp2_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)  \n",
    "        self.fc_logvar = nn.Linear(64, latent_dim) \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim + tp2_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c, tp2):\n",
    "        xctp2 = torch.cat([x, c, tp2], dim=1)\n",
    "        h = self.encoder(xctp2)\n",
    "        mu = self.fc_mu(h) \n",
    "        logvar = self.fc_logvar(h)  \n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c, tp2):\n",
    "        zctp2 = torch.cat([z, c, tp2], dim=1)\n",
    "        return self.decoder(zctp2)\n",
    "\n",
    "    def forward(self, x, c, tp2):\n",
    "        mu, logvar = self.encode(x, c, tp2)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z, c, tp2)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa454f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cVAETrainer:\n",
    "   def __init__(self, input_dim, latent_dim, cond_dim, tp2_dim, lr=0.001):\n",
    "       self.input_dim = input_dim\n",
    "       self.latent_dim = latent_dim\n",
    "       self.cond_dim = cond_dim\n",
    "       self.tp2_dim = tp2_dim\n",
    "       # Initialize model\n",
    "       self.model = cVAE(input_dim, latent_dim, cond_dim, tp2_dim)\n",
    "       self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "       self.loss_fn = nn.MSELoss()  # Reconstruction loss\n",
    "   def train(self, X_train, y_train, tp2_train, epochs, batch_size):\n",
    "       X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "       y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "       tp2_train = torch.tensor(tp2_train, dtype=torch.float32)\n",
    "       y_train_onehot = torch.nn.functional.one_hot(y_train, num_classes=self.cond_dim).float()\n",
    "       dataset = TensorDataset(X_train, y_train_onehot, tp2_train)\n",
    "       dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "       for epoch in range(epochs):\n",
    "           total_loss = 0.0\n",
    "           for batch_X, batch_c, batch_tp2 in dataloader:\n",
    "               # Forward propagation\n",
    "               recon_X, mu, logvar = self.model(batch_X, batch_c, batch_tp2)\n",
    "               # Calculate loss\n",
    "               recon_loss = self.loss_fn(recon_X, batch_X)  # Reconstruction loss\n",
    "               kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL divergence\n",
    "               loss = recon_loss + kl_divergence\n",
    "               # Backward propagation\n",
    "               self.optimizer.zero_grad()\n",
    "               loss.backward()\n",
    "               self.optimizer.step()\n",
    "               total_loss += loss.item()\n",
    "#             print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(dataloader):.4f}\")\n",
    "   def generate_samples(self, num_samples, minority_class, tp2_samples):\n",
    "       self.model.eval()\n",
    "       # Generate class labels\n",
    "       new_c = torch.ones(num_samples, dtype=torch.long) * minority_class\n",
    "       new_c = torch.nn.functional.one_hot(new_c, num_classes=self.cond_dim).float()\n",
    "       # Generate random noise\n",
    "       noise = torch.randn(num_samples, self.latent_dim)\n",
    "       # Convert tp2_samples to torch.Tensor\n",
    "       tp2_samples = torch.tensor(tp2_samples, dtype=torch.float32)\n",
    "       # Generate new samples\n",
    "       with torch.no_grad():\n",
    "           fake_data = self.model.decode(noise, new_c, tp2_samples)\n",
    "       return fake_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf3fde7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Pseudoflavonifractor_sp_An184', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Streptococcus_vestibularis', 'Enterococcus_faecalis', 'Streptococcus_sp_F0442', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Sellimonas_intestinalis', 'Clostridium_perfringens', 'Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Denitrobacterium_detoxificans', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Enterococcus_faecium', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila', 'Enterococcus_faecalis', 'Blautia_producta']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5700 ± 0.1294\n",
      "  Sensitivity: 0.2400 ± 0.2752\n",
      "  Specificity: 0.9000 ± 0.0782\n",
      "  AUC: 0.6457 ± 0.1160\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    extracted_data = clr_transform(extracted_data)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    original_num_features = extracted_data_final.shape[2]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)  \n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    tp2_minority = tp2_train[minority_indices]\n",
    "    \n",
    "    repeat_times = num_new_samples // len(tp2_minority) + 1\n",
    "    tp2_minority = np.tile(tp2_minority, (repeat_times, 1))[:num_new_samples]\n",
    "\n",
    "    latent_dim = 10\n",
    "    input_dim = X_train.shape[1]\n",
    "    cond_dim = 2  # two classes\n",
    "    cvae = cVAETrainer(input_dim, latent_dim, cond_dim, tp2_train.shape[1])  \n",
    "\n",
    "    cvae.train(X_train, y_train, tp2_train, epochs=100, batch_size=16)\n",
    "\n",
    "    new_samples_np = cvae.generate_samples(num_new_samples, minority_class=1, tp2_samples=tp2_minority)\n",
    "    new_samples_np = new_samples_np.reshape(num_new_samples, -1)\n",
    "\n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(num_new_samples, minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41eaeb5",
   "metadata": {},
   "source": [
    "### Kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c65f8",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/60223573/conditional-sampling-from-multivariate-kernel-density-estimate-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8b704e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateConditionalKDE:\n",
    "    \"\"\"\n",
    "    Multivariate Conditional KDE for generating new X samples conditioned on tp2 and class label.\n",
    "    This implementation learns from all classes but generates only minority class samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bandwidth_factor=1.0):\n",
    "        \n",
    "        self.bandwidth_factor = bandwidth_factor\n",
    "        self.X = None  # Features\n",
    "        self.tp2 = None  # Conditional variables\n",
    "        self.y = None  # Labels\n",
    "        self.X_scaler = StandardScaler()\n",
    "        self.tp2_scaler = StandardScaler()\n",
    "        self.class_indices = {}  # Dictionary to store indices for each class\n",
    "        \n",
    "    def fit(self, X, y, tp2, minority_class=1):\n",
    "        \"\"\"\n",
    "        Fit the conditional KDE model using all samples but remembering class information.\n",
    "        \"\"\"\n",
    "        self.y = np.asarray(y)\n",
    "        \n",
    "        # Scale all data (using entire dataset)\n",
    "        self.X = self.X_scaler.fit_transform(X)\n",
    "        self.tp2 = self.tp2_scaler.fit_transform(tp2)\n",
    "        \n",
    "        # Store indices for each class\n",
    "        unique_classes = np.unique(y)\n",
    "        for cls in unique_classes:\n",
    "            self.class_indices[cls] = np.where(y == cls)[0]\n",
    "        \n",
    "        # Store minority class for later use\n",
    "        self.minority_class = minority_class\n",
    "        \n",
    "        # Compute the bandwidth using Silverman's rule\n",
    "        n_samples = self.X.shape[0]\n",
    "        self.X_bw = self.bandwidth_factor * np.std(self.X, axis=0) * (n_samples ** (-1/5))\n",
    "        self.tp2_bw = self.bandwidth_factor * np.std(self.tp2, axis=0) * (n_samples ** (-1/5))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_weights(self, tp2_condition, target_class):\n",
    "        \"\"\"\n",
    "        Compute weights for each training point of the specified class\n",
    "        based on similarity to the conditional tp2 value.\n",
    "        \"\"\"\n",
    "        # Get indices for the target class\n",
    "        class_indices = self.class_indices[target_class]\n",
    "        \n",
    "        # Normalize the condition\n",
    "        tp2_normalized = self.tp2_scaler.transform(tp2_condition.reshape(1, -1))\n",
    "        \n",
    "        # Compute distances in tp2 space for the specified class\n",
    "        class_tp2 = self.tp2[class_indices]\n",
    "        distances = cdist(tp2_normalized, class_tp2, 'euclidean')\n",
    "        \n",
    "        # Apply kernel function to get weights\n",
    "        weights = np.exp(-0.5 * distances ** 2 / np.mean(self.tp2_bw) ** 2)\n",
    "        weights = weights.flatten()\n",
    "        \n",
    "        # Normalize weights\n",
    "        if np.sum(weights) > 0:\n",
    "            weights = weights / np.sum(weights)\n",
    "        \n",
    "        return weights, class_indices\n",
    "    \n",
    "    def generate_samples(self, tp2_conditions, n_samples_per_condition=1):\n",
    "        \"\"\"\n",
    "        Generate new X samples of the minority class conditional on given tp2 values.\n",
    "        \"\"\"   \n",
    "        all_samples = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for tp2_condition in tp2_conditions:\n",
    "            # Compute weights for the minority class\n",
    "            weights, minority_indices = self._compute_weights(tp2_condition, self.minority_class)\n",
    "            \n",
    "            # Generate samples by weighted resampling with perturbation\n",
    "            for _ in range(n_samples_per_condition):\n",
    "                # Sample a point based on weights\n",
    "                idx = np.random.choice(len(minority_indices), p=weights)\n",
    "                original_idx = minority_indices[idx]\n",
    "                base_point = self.X[original_idx]\n",
    "                \n",
    "                # Add Gaussian noise scaled by bandwidth\n",
    "                noise = np.random.normal(0, self.X_bw)\n",
    "                new_sample = base_point + noise\n",
    "                \n",
    "                all_samples.append(new_sample)\n",
    "                all_labels.append(self.minority_class)\n",
    "        \n",
    "        # Convert back to original scale\n",
    "        new_X_samples = self.X_scaler.inverse_transform(np.array(all_samples))\n",
    "        new_y_labels = np.array(all_labels)\n",
    "        \n",
    "        return new_X_samples, new_y_labels\n",
    "\n",
    "\n",
    "def conditional_kde(X_train, y_train, tp2_train, minority_class=1, num_new_samples=100, bandwidth_factor=1.0):\n",
    "    \"\"\"\n",
    "    Generate new samples for the minority class conditioned on tp2 values.\n",
    "    This function learns from all classes but generates only minority class samples.\n",
    "    \"\"\"\n",
    "    ckde = MultivariateConditionalKDE(bandwidth_factor=bandwidth_factor)\n",
    "    ckde.fit(X_train, y_train, tp2_train, minority_class=minority_class)\n",
    "    \n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    selected_indices = np.random.choice(minority_indices, num_new_samples, replace=True)\n",
    "    selected_tp2 = tp2_train[selected_indices]\n",
    "    \n",
    "    new_samples, new_labels = ckde.generate_samples(\n",
    "        selected_tp2, \n",
    "        n_samples_per_condition=1\n",
    "    )\n",
    "    \n",
    "    return new_samples, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6f719a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Pseudoflavonifractor_sp_An184', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Streptococcus_vestibularis', 'Enterococcus_faecalis', 'Streptococcus_sp_F0442', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Sellimonas_intestinalis', 'Clostridium_perfringens', 'Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Denitrobacterium_detoxificans', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Enterococcus_faecium', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila', 'Enterococcus_faecalis', 'Blautia_producta']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.6162 ± 0.0783\n",
      "  Sensitivity: 0.5467 ± 0.2647\n",
      "  Specificity: 0.6857 ± 0.1616\n",
      "  AUC: 0.6362 ± 0.1054\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    extracted_data = clr_transform(extracted_data)\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "    \n",
    "#     print(f\"train set: sum={len(y_train)}, minority={minority_count}, majority={majority_count}\")\n",
    "    \n",
    "    new_samples, new_labels = conditional_kde(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "#     print(f\"Generated {len(new_samples)} new samples\")\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        auc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "        metrics = {\n",
    "            'Balanced_accuracy': balanced_accuracy,\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Specificity': specificity,\n",
    "            'AUC': auc,\n",
    "        }\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29bf7d",
   "metadata": {},
   "source": [
    "### Dirichlet Distribution + Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f264b65",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/shizheng_Li/article/details/144152742"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c603424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_dirichlet(X_train, y_train, tp2_train, minority_class=1, \n",
    "                        num_new_samples=55, a_prior=0.5, b_prior=0.5):\n",
    "   X_scaler = StandardScaler()\n",
    "   X_scaled = X_scaler.fit_transform(X_train)\n",
    "   \n",
    "   tp2_scaler = StandardScaler()\n",
    "   tp2_scaled = tp2_scaler.fit_transform(tp2_train)\n",
    "   \n",
    "   # Ensure all features are positive (requirement for Dirichlet distribution)\n",
    "   X_min = np.min(X_scaled, axis=0)\n",
    "   X_positive = np.maximum(X_scaled - X_min + 1e-3, 1e-8)\n",
    "   \n",
    "   # Get indices of minority class samples\n",
    "   minority_indices = np.where(y_train == minority_class)[0]\n",
    "   tp2_minority = tp2_scaled[minority_indices]\n",
    "   \n",
    "   # Randomly select conditional values from minority class\n",
    "   selected_indices = np.random.choice(len(minority_indices), num_new_samples, replace=True)\n",
    "   selected_tp2 = tp2_minority[selected_indices]\n",
    "   \n",
    "   new_samples = []\n",
    "   \n",
    "   # Get class information for analyzing distributions based on y\n",
    "   unique_classes = np.unique(y_train)\n",
    "   class_indices = {cls: np.where(y_train == cls)[0] for cls in unique_classes}\n",
    "   \n",
    "   # Generate samples for each selected tp2 condition\n",
    "   for tp2_condition in selected_tp2:\n",
    "       # Calculate weights based on tp2 condition - using all samples\n",
    "       tp2_distances = np.sum((tp2_scaled - tp2_condition) ** 2, axis=1) ** 0.5\n",
    "       tp2_distances = np.maximum(tp2_distances, 1e-8)\n",
    "       tp2_weights = 1.0 / tp2_distances\n",
    "       tp2_weights = tp2_weights / np.sum(tp2_weights)\n",
    "       \n",
    "       # Calculate alpha parameter for each feature dimension\n",
    "       alphas = np.zeros(X_positive.shape[1])\n",
    "       \n",
    "       for j in range(X_positive.shape[1]):\n",
    "           # Based on tp2 condition, analyze feature distributions for each class\n",
    "           class_feature_stats = {}\n",
    "           for cls in unique_classes:\n",
    "               cls_indices = class_indices[cls]\n",
    "               cls_features = X_positive[cls_indices, j]\n",
    "               cls_weights = tp2_weights[cls_indices]\n",
    "               \n",
    "               # Calculate weighted statistics for each class\n",
    "               weighted_mean = np.sum(cls_weights * cls_features) / np.sum(cls_weights) if np.sum(cls_weights) > 0 else 0\n",
    "               class_feature_stats[cls] = {\n",
    "                   'mean': weighted_mean,\n",
    "                   'indices': cls_indices,\n",
    "                   'weights': cls_weights\n",
    "               }\n",
    "           \n",
    "           # Calculate feature difference ratio between classes\n",
    "           if len(unique_classes) > 1 and minority_class in class_feature_stats:\n",
    "               # Calculate ratio of feature means to represent class differences\n",
    "               minority_mean = class_feature_stats[minority_class]['mean']\n",
    "               \n",
    "               # Calculate mean of all other classes\n",
    "               other_classes = [c for c in unique_classes if c != minority_class]\n",
    "               other_means = [class_feature_stats[c]['mean'] for c in other_classes]\n",
    "               other_mean = np.mean(other_means) if other_means else 0\n",
    "               \n",
    "               # Calculate class difference ratio\n",
    "               class_ratio = minority_mean / (other_mean + 1e-8)\n",
    "               class_ratio = np.clip(class_ratio, 0.5, 2.0)  # Limit ratio range\n",
    "           else:\n",
    "               class_ratio = 1.0\n",
    "           \n",
    "           # Calculate weighted posterior parameters using all samples\n",
    "           weighted_data = np.sum(tp2_weights * X_positive[:, j])\n",
    "           weighted_log_data = np.sum(tp2_weights * np.log(X_positive[:, j]))\n",
    "           \n",
    "           # Bayesian posterior\n",
    "           a_posterior = a_prior + weighted_data\n",
    "           b_posterior = b_prior + weighted_log_data\n",
    "           \n",
    "           # Use expected value of posterior as alpha\n",
    "           alpha_j = a_posterior / b_posterior\n",
    "           \n",
    "           # Adjust alpha based on class feature differences to make it closer to minority class distribution\n",
    "           alpha_j *= class_ratio\n",
    "           \n",
    "           # Ensure alpha is positive\n",
    "           alphas[j] = max(alpha_j, 0.01)\n",
    "       \n",
    "       sample = np.random.dirichlet(alphas)\n",
    "       new_samples.append(sample)\n",
    "   \n",
    "   new_samples = np.array(new_samples)\n",
    "   \n",
    "   new_samples_scaled = new_samples * (np.max(X_scaled, axis=0) - X_min) + X_min\n",
    "   \n",
    "   new_samples_orig = X_scaler.inverse_transform(new_samples_scaled)\n",
    "   \n",
    "   new_labels = np.full(num_new_samples, minority_class)\n",
    "   \n",
    "   return new_samples_orig, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3830339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Pseudoflavonifractor_sp_An184', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Streptococcus_vestibularis', 'Enterococcus_faecalis', 'Streptococcus_sp_F0442', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Sellimonas_intestinalis', 'Clostridium_perfringens', 'Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Denitrobacterium_detoxificans', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Enterococcus_faecium', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila', 'Enterococcus_faecalis', 'Blautia_producta']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.6190 ± 0.1083\n",
      "  Sensitivity: 0.3467 ± 0.2400\n",
      "  Specificity: 0.8914 ± 0.0860\n",
      "  AUC: 0.6676 ± 0.1030\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    extracted_data = clr_transform(extracted_data)\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    new_samples, new_labels = conditional_dirichlet(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=1,\n",
    "        num_new_samples=num_new_samples,\n",
    "        a_prior=0.5, \n",
    "        b_prior=0.8\n",
    "    )\n",
    "\n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1827de1",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4e56804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Pseudoflavonifractor_sp_An184', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Streptococcus_vestibularis', 'Enterococcus_faecalis', 'Streptococcus_sp_F0442', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Sellimonas_intestinalis', 'Clostridium_perfringens', 'Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Allisonella_histaminiformans', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Denitrobacterium_detoxificans', 'Enterococcus_faecalis', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Lactococcus_lactis', 'Allisonella_histaminiformans', 'Enterococcus_faecium', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila', 'Enterococcus_faecalis', 'Blautia_producta']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5914 ± 0.1130\n",
      "  Sensitivity: 0.3800 ± 0.2495\n",
      "  Specificity: 0.8029 ± 0.1191\n",
      "  AUC: 0.6210 ± 0.1007\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    extracted_data = clr_transform(extracted_data)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X_all = extracted_data_final[:, :, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "\n",
    "    X_train_all = X_all[train_idx]\n",
    "    X_val_all = X_all[val_idx]\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    original_num_features = extracted_data_final.shape[2]\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    X_train_minority = X_train[minority_indices]\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    combined_X_train, combined_y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5c699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c123b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
